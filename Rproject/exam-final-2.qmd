---
title: "Final Exam" 
author: "Alex Yang"
format: ds6030hw-html
editor: 
  markdown: 
    wrap: 72
---

::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em; padding-bottom: .5em;"}
This is an **independent assignment**. Do not discuss or work with
classmates.
:::

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Grading Notes

-   The exam is graded out of 100 pts.
-   20 points are given for overall style and ease of reading. If you
    don't use the homework format or print out pages of unnecessary
    output the style points will be reduced.
-   The point totals for each question are provided below.
-   Be sure to show your work so you can get partial credit even if your
    solution is wrong.

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data' # data directory
library(tidyverse) # functions for data manipulation   
library(mclust)    # model based clustering
library(mixtools)  # for poisson mixture models
```

# Problem 1: Customer Segmentation (15 pts)

RFM analysis is an approach that some businesses use to understand their
customers' activities. At any point in time, a company can measure how
recently a customer purchased a product (Recency), how many times they
purchased a product (Frequency), and how much they have spent (Monetary
Value). There are many ad-hoc attempts to segment/cluster customers
based on the RFM scores (e.g., here is one based on using the customers'
rank of each dimension independently:
<https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>).
In this problem you will use the clustering methods we covered in class
to segment the customers.

The data for this problem can be found here:
\<`r file.path(data_dir, "RFM.csv")`\>. Cluster based on the `Recency`,
`Frequency`, and `Monetary` features.

## a. Load the data (3 pts)

::: {.callout-note title="Solution"}
```{r}
# loading the data 
customer_data = read.csv(file.path(data_dir, "RFM.csv"))
```

```{r}
# Getting a quick view of the data 
str(customer_data)
summary(customer_data)
cat("\n Missing data count: \n")
colSums(is.na(customer_data))
```

```{r}
# Visualize the data distribution 
ggplot(customer_data, aes(x = Recency)) + 
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) + 
  ggtitle("Distribution of Recency")

ggplot(customer_data, aes(x = Frequency)) + 
  geom_histogram(bins = 30, fill = "green", alpha = 0.7) + 
  scale_x_continuous(breaks = seq(0,25, by =1))+
  ggtitle("Distribution of Frequency")

ggplot(customer_data, aes(x = Monetary)) + 
  geom_histogram(bins = 30, fill = "orange", alpha = 0.7) + 
  ggtitle("Distribution of Monetary Value")
```
:::

## b. Implement hierarchical clustering. (3 pts)

-   Describe any pre-processing steps you took (e.g., scaling, distance
    metric)
-   State the linkage method you used with justification.
-   Show the resulting dendrogram.
-   State the number of segments/clusters you used with justification.
-   Using your segmentation, are customers 1 and 12 in the same cluster?

::: {.callout-note title="Solution"}
```{r}
# Step 1: Pre-processing
#   Scaling the data
scaled_data <- customer_data %>%
  select(-id) %>%    # Remove the 'id' column
  scale()            # Scale the remaining columns

#   Distance Metric (Euclidean distance)
distance_matrix <- dist(scaled_data, method = "euclidean")



# Step 2: Linkage method 
hclust_model <- hclust(distance_matrix, method = "ward.D2")
hc <- hclust(distance_matrix, method = "ward.D2")
# Justification: I used ward.d2 method because it minimizes the total within cluster variance. Since we are trying to understand the customer's activities we don't want to have too many small groups. Ward's linkage would produce clusters of roughly similar sizes avoiding small clusters being merged into large ones prematurely as well. Ward's linkage works very well with numerical data too and we aim to minimize variance in this particular problem. 



# Step 3: show the resulting denodrogram
colPalette <- c('#1b9e77', '#d95f02', 'red', 'blue', '#66a61e') 

#   Cut the dendrogram into clusters (example: k = 4)
clusters <- cutree(hc, k = 4)

#   Plot the dendrogram 
plot(as.dendrogram(hc), 
     las = 1,           
     leaflab = "none",   
     ylab = "Height",    
     main = "Dendrogram hc") 

#   Order and customize points below the dendrogram
ord <- hc$order                  
labels <- clusters[ord]           
colors <- colPalette[labels]     
shapes <- 15                     
n <- length(labels)            

#   Add points below the dendrogram
points(1:n, rep(0, n), col = colors, pch = shapes, cex = 0.8)

# Calculating the eblow data
# Calculate merge heights and cluster numbers
elbow_data <- tibble(
  height = rev(hc$height),               # Heights of merges (reversed)
  K = seq_along(hc$height) + 1          # Cluster numbers (2 to n)
)

#   Add a horizontal line at k==4$height to indicate clusters 
abline(h = filter(elbow_data, K==4)$height, lty = 3, col = "grey40")



# Step 4: Number of Segments
#   Calculate merge heights and cluster numbers
elbow_data <- tibble(
  height = hc$height,               # Heights of merges
  K = row_number(-hc$height)       # Cluster numbers (reverse order)
)

#   Plot the elbow method graph (raw height)
elbow_data %>%
  ggplot(aes(K, height)) +
  geom_line() +
  geom_point(aes(color = ifelse(K == 4, "red", "black"))) +  
  scale_color_identity() +                               
  coord_cartesian(xlim = c(1, 30)) +                        
  labs(title = "Elbow Method for Choosing K (Raw Height)",
       x = "Number of Clusters (K)",
       y = "Height")

# Explanation: I used the elbow method and I generate a graph to see the height when there is a large jump in the height it indicates that merging those clusters caused a significant loss of cluster compactness. The optimal k or the k I chose in this situation is located right before the big jump. 



# Step 5: Are Customer 1 and 12 in the same cluster? 
clusters <- cutree(hc, k = 4)
#   Check the cluster assignments for Customer 1 and Customer 12
clusters[1] == clusters[12]
cat("Customer 1 and 12 are in the same cluster")
```
:::

## c. Implement k-means. (3 pts)

-   Describe any pre-processing steps you took (e.g., scaling)
-   State the number of segments/clusters you used with justification.
-   Using your segmentation, are customers 1 and 12 in the same cluster?

::: {.callout-note title="Solution"}
```{r}
# Step 1: Pre-processing (sclaing the data)
scaled_data <- scale(customer_data %>% select(-id)) 

#   Outlier detection by Mahalanobis Distance
mahal_dist <- mahalanobis(scaled_data, colMeans(scaled_data), cov(scaled_data))
threshold <- qchisq(0.99, df = ncol(scaled_data)) #set threashold using chi-squared distribution
outliers <- which(mahal_dist > threshold)  # identify outlier based on threashold
scaled_data_clean <- scaled_data[-outliers, ] # deleting outliers from data

# Step 2: number of segments 
#   Calculate WCSS for different numbers of clusters
wcss <- sapply(1:10, function(k) {
  set.seed(123)
  kmeans(scaled_data, centers = k, nstart = 25)$tot.withinss
})

#   Plot the Elbow Method
wcss_data <- tibble(
  k = 1:10,
  wcss = wcss
)

ggplot(wcss_data, aes(x = k, y = wcss)) +
  geom_line() +
  geom_point(aes(color = ifelse(k == 4, "red", "black")), size = 3) +  # Highlight k=4
  scale_color_identity() +
  labs(
    title = "Elbow Method for Optimal Number of Clusters",
    x = "Number of Clusters (k)",
    y = "Within-Cluster Sum of Squares (WCSS)"
  )
# Justification: 

# Step 3: Implement k-means clustering 
kmeans_model <- kmeans(scaled_data, centers = 4, nstart = 25, iter.max = 3000)
kmeans_clusters <- kmeans_model$cluster # getting cluster assignment for each customer

# Step 4: customer 1 and 12 clustering? 
cat('customer 1 and 12 are not in the same cluster group')
kmeans_clusters[1] == kmeans_clusters[12]
```
:::

## d. Implement model-based clustering (3 pts)

-   Describe any pre-processing steps you took (e.g., scaling)
-   State the number of segments/clusters you used with justification.
-   Describe the best model. What restrictions are on the shape of the
    components?
-   Using your segmentation, are customers 1 and 100 in the same
    cluster?

::: {.callout-note title="Solution"}
```{r}
library(mclust)

# Step 1: Pre-processing (scaling the data)
# already done in part b and c to scale and get rid of the outliers

# Step 2: Implement Model-Based Clustering
# Fit the Gaussian Mixture Model
mclust_model <- Mclust(scaled_data)
summary(mclust_model)

# Step 3: Selecting the Best Model
#   Use BIC to determine the best model
best_model <- mclust_model$modelName
best_clusters <- mclust_model$classification
plot(mclust_model, what="BIC")
cat("The best model is:", best_model, "\n")

# Step 4: Number of Segments/Clusters
optimal_k <- length(unique(best_clusters))
cat("The number of clusters chosen is:", optimal_k, "\n")

# Justification: 
# The model with the highest BIC was selected as it provides the best trade-off between model fit and complexity.
# Restrictions on the shape of components: The Gaussian Mixture Model assumes that clusters are Gaussian-shaped, and each component can have different volume, shape, and orientation depending on the model (e.g., spherical, diagonal, or full covariance).

# Step 5: Checking if Customers 1 and 100 are in the same cluster
same_cluster <- best_clusters[1] == best_clusters[100]
cat("Are Customers 1 and 100 in the same cluster?", same_cluster, "\n")

plot(mclust_model, what = "density")
```
:::

## e. Discussion of results (3 pts)

Discuss how you would cluster the customers if you had to do this for
your job. Do you think one model would do better than the others?

::: {.callout-note title="Solution"}
> I believe that model-based clustering would be the best approach 
:::

# Problem 2: Unbalanced Data (15 pts)

A researcher is trying to build a predictive model for distinguishing
between real and AI generated images. She collected a random sample
($n=10,000$) of tweets/posts that included images. Expert analysts were
hired to label the images as real or AI generated. They determined that
1000 were AI generated and 9000 were real.

She tasked her grad student with building a logistic regression model to
predict the probability that a new image is AI generated. After reading
on the internet, the grad student became concerned that the data was
*unbalanced* and fit the model using a weighted log-loss $$
-\sum_{i=1}^n w_i \left[ y_i \log \hat{p}(x_i) + (1-y_i) \log (1-\hat{p}(x_i)) \right]
$$ where $y_i = 1$ if AI generated ($y_i=0$ if real) and $w_i = 1$ if
$y_i = 1$ (AI) and $w_i = 1/9$ if $y_i = 0$ (real). This makes
$\sum_i w_iy_i = \sum_i w_i(1-y_i) = 1000$. That is the total weight of
the AI images equals the total weight of the real images. Note: An
similar alternative is to downsample the real images; that is, build a
model with 1000 AI and a random sample of 1000 real images. The grad
student fits the model using the weights and is able to make predictions
$\hat{p}(x)$.

While the grad student is busy implementing this model, the researcher
grabbed another 1000 random tweets/posts with images and had the experts
again label them real or AI. Excitedly, the grad student makes
predictions on the test data. However, the model doesn't seem to be
working well on these new test images. While the AUC appears good, the
log-loss and brier scores are really bad.

Hint: By using the weights (or undersampling), the grad student is
modifying the base rate (prior class probability).

## a. What is going on? (5 pts)

How can the AUC be strong while the log-loss and brier scores aren't.

::: {.callout-note title="Solution"}
## Modifications done to base rate

-   at the beginning the grad student was provided with a dataset that
    has 10% AI generated images and 90% human generate images.
-   However, the weighting that was added later made the dataset into a
    50-50 AI generated images and human generate images
-   The grad student used this weighted dataset to train the logit
    regression which is unable to distinguish the weighted data from the
    population data.

## Implementations on the new test set

-   since the model is trained on the dataset that doesn't represent the
    actual population; what it represents is the population that has 50%
    AI generateed images and 50% human generated image. When the grad
    student used this particular inaccurate model on the test data set
    the probability is very off.

## Why AUC strong while the log-loss and brier scores aren't

-   The AUC is a performance metric that measure the ability of a model
    to rank predictions properly which is different from the actual
    predicted probability (aka log-loos and brier scores). The
    undersampling or weighted model only affects the predicted
    probability but it doesn't actually affect the ranked order of
    predictions
:::

## b. What is the remedy? (5 pts)

Specifically, how should the grad student adjust the predictions for the
new test images? Use equations and show your work. Hints: the model is
outputting $\hat{p}(x) = \widehat{\Pr}(Y=1|X=x)$; consider the log odds
and Bayes theorem.

::: {.callout-note title="Solution"}
![answer for remedy](remedyanswer.png){width=100, height =1000}
:::

## c. Base rate correction (5 pts)

If the grad student's weighted model predicts an image is AI generated
with $\hat{p}(x) = .80$, what is the updated prediction under the
assumption that the true proportion of AI is 1/10.

::: {.callout-note title="Solution"}
We can apply the formula we derived from part b to answer this question

$$
p^*(x) = \frac{0.1\times0.8}{0.1\times0.8+0.9\times0.2}\approx0.308
$$
:::

# Problem 3: Multiclass Classification (10 pts)

You have built a predictive model that outputs a probability vector
$\hat{p}(x) = [\hat{p}_1(x), \hat{p}_2(x), \hat{p}_3(x)]$ for a 3-class
categorical output. Consider the following loss matrix which includes an
option to return *No Decision* if there is too much uncertainty in the
label:

|         | $\hat{G} =1$ | $\hat{G} =2$ | $\hat{G} =3$ | No Decision |
|:--------|-------------:|-------------:|-------------:|------------:|
| $G = 1$ |            0 |            2 |            2 |           1 |
| $G = 2$ |            1 |            0 |            2 |           1 |
| $G = 3$ |            1 |            1 |            0 |           1 |

What label would you output if the estimated probability is:
$\hat{p}(x) = [0.25, 0.15, 0.60]$. Show your work.

::: {.callout-note title="Solution"}
We are given the estimated probability vector: $$
\hat{p}(x) = [0.25, 0.15, 0.60],
$$

We know that expected loss = the sum of (probability X loss) for each
class or $$
\text{Expected Loss}(\hat{G}) = \sum_{G=1}^3 \hat{p}_G(x) \cdot \text{Loss}(G| \hat{G}),
$$

For $\hat{G} = 1$:
$$E[L|\hat{G}=1] = 0(0.25) + 1(0.15) + 1(0.60) = 0.75$$ For
$\hat{G} = 2$: $$E[L|\hat{G}=2] = 2(0.25) + 0(0.15) + 1(0.60) = 1.10$$
For $\hat{G} = 3$:
$$E[L|\hat{G}=3] = 2(0.25) + 2(0.15) + 0(0.60) = 0.80$$ For No Decision:
$$E[L|\text{No Decision}] = 1(0.25) + 1(0.15) + 1(0.60) = 1.00$$
Comparing all the expected losses we find that $\hat{G} = 1$ because it
has the lowest expected loss of $0.75$
:::

# Problem 4: Donor Acceptance Modeling (40 pts)

::: {style="background-color:blue; color:yellow; display: block; border-color: black; padding:1em; padding-bottom: .5em;"}
The data for this problem is for your private use on this exam only. You
may not share or use for any other purposes.
:::

This challenge has you predicting the probability that a pediatric donor
heart offer will be Accepted or Rejected. Use the
`donor_accept_train.csv` data (available in Canvas) to build a model to
predict the probability of `outcome = "Accept"`. The test data
`donor_accept_test.csv` is used for making predictions.

A description of the transplant system and variables is provided in
`donor_accept_vars.html`.

Hints:

-   There are four parts to this problem. Before you being think about
    how your approach will address all four (for example, your choice of
    model(s) in part a may influence your approach to part c).

-   As always, *before you start coding* write out each step of the
    process. Think about inputs and outputs.

## a. Probability Prediction Contest (10 pts)

Build a model to predict the probability that an offer will be accepted.
Performance is evaluated using log-loss.

*Contest Submission:*

-   Submit your predictions on the `donor_accept_test.csv` data. Create
    a .csv file (ensure comma separated format) named
    `lastname_firstname.csv` that includes the column named
    "prob_accept" that is your estimated posterior probability. We will
    use automated evaluation, so the format must be exact.

*Notes:*

-   I suggest you quickly make an initial model without doing any
    feature engineering or much tuning. There are a lot of features, an
    endless number of feature engineering tasks, many predictive models
    each with many tuning parameters to choose from. But just get
    something that correctly outputs probabilities and use it to
    complete the other parts to this problem. You can always come back
    and improve the model if your time permits.

-   You must show your code. Because your code may take some time to
    run, you may want to run the model outside this notebook. If you do
    so, copy the final code into this notebook and set `eval=FALSE` in
    the corresponding code chunk(s) so we can see the code, but it won't
    run when the notebook compiles.

*Competition Grading:*

-   2 of the 10 points are based on readable code
-   3 of the 10 points are based on a valid submission (e.g., correct
    number of rows and log-loss beats an intercept only model)
-   The remaining 5 points are based on your predictive performance. The
    top score will receive all 5, the second best 4.93, third best 4.85,
    etc.

::: {.callout-note title="Solution"}
### a.1 Loading and Inspecting Data

```{r}
library(tidyverse)
library(caret)
library(xgboost)
library(recipes)
library(gridExtra)

# Loading Data
train_data <- read.csv("donor_accept_train-1.csv")
test_data <- read.csv("donor_accept_test.csv")

# Inspecting Data 
head(train_data, n = 10)

# Plot the class distribution
ggplot(train_data, aes(x = outcome)) +
  geom_bar(fill = "blue") +
  theme_minimal() +
  labs(title = "Distribution of Acceptance vs Rejection",
       x = "Outcome",
       y = "Count")

# Check for NA values
na_counts <- colSums(is.na(train_data))
na_props <- data.frame(
  variable = names(na_counts),
  proportion = na_counts / nrow(train_data)
) %>%
  filter(proportion > 0) %>%  # Only keep variables with missing values
  arrange(desc(proportion))

ggplot(na_props, aes(x = reorder(variable, proportion), y = proportion)) +
  geom_bar(stat = "identity", fill = "coral") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Proportion of Missing Values",
       x = "Variable",
       y = "Proportion Missing") +
  scale_y_continuous(labels = scales::percent_format())
```

### a.2 Data Processing/Feature Engineering

```{r}
# Transforming outcomes to 0 and 1 for computing
train_data$outcome <- ifelse(train_data$outcome == "Accept", 1, 0)

# Define pre-processing recipe
data_recipe <- recipe(outcome ~ ., data = train_data) %>%
  step_rm(OFFER_ID) %>%  # Remove non-predictive identifier
  step_impute_median(all_numeric_predictors()) %>%  # Impute missing values in numeric columns
  step_impute_mode(all_nominal_predictors()) %>%  # Impute missing values in categorical columns
  step_dummy(all_nominal_predictors()) %>%  # Convert categorical columns to dummy variables
  step_center(all_numeric_predictors()) %>%  # Center numeric columns
  step_scale(all_numeric_predictors())       # Scale numeric columns

```

### a.3 Prepping Data and Feature Importance

```{r}
# Prep and bake data
prep_data <- prep(data_recipe, training = train_data)
train_processed <- bake(prep_data, new_data = NULL)
test_processed <- bake(prep_data, new_data = test_data)

# Initial feature importance calculation
initial_train_matrix <- as.matrix(train_processed %>% select(-outcome))
initial_train_labels <- train_processed$outcome
initial_dtrain <- xgb.DMatrix(data = initial_train_matrix, label = initial_train_labels)

# Train initial model for feature importance
initial_model <- xgb.train(
  params = list(
    objective = "binary:logistic",
    eval_metric = "logloss"
  ),
  data = initial_dtrain,
  nrounds = 500
)

# Get feature importance
importance <- xgb.importance(model = initial_model)
importance_df <- as.data.frame(importance)

# Select top features (adjust threshold as needed)
feature_threshold <- 0.01  # Keep features with gain > 1%
selected_features <- importance_df$Feature[importance_df$Gain > feature_threshold]

# Filter data to include only selected features
train_matrix <- as.matrix(train_processed %>% 
                           select(all_of(c(selected_features, "outcome"))) %>%
                           select(-outcome))
train_labels <- train_processed$outcome

test_matrix <- as.matrix(test_processed %>% 
                          select(all_of(selected_features)))

# Create final training matrix
dtrain <- xgb.DMatrix(data = train_matrix, label = train_labels)

```

### a.4 Model Training and Evaluation

```{r}
set.seed(42)
# Define XGBoost parameters
xgb_params <- list(
  objective = "binary:logistic",
  eval_metric = "logloss",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8
)

# Cross-validation with selected features
cv_model <- xgb.cv(
  params = xgb_params,
  data = dtrain,
  nrounds = 1000, # change this to reduce computing time
  nfold = 5,
  early_stopping_rounds = 20,
  verbose = 0
)

best_nrounds <- cv_model$best_iteration
cat("Best number of rounds:", best_nrounds, "\n")

# Train final model with selected features
xgb_model <- xgb.train(
  params = xgb_params,
  data = dtrain,
  nrounds = best_nrounds,
  verbose = 0
)
```

### a.5 Model Prediction

```{r}
# Predict on test data
dtest <- xgb.DMatrix(data = test_matrix)
predictions <- predict(xgb_model, dtest)

# Create submission file
submission <- data.frame(
  OFFER_ID = test_data$OFFER_ID,
  prob_accept = predictions
)

write.csv(submission, "yang_zhe_with_feature_selection.csv", row.names = FALSE)  

# Visualize final feature importance
final_importance <- xgb.importance(model = xgb_model)
final_importance_df <- as.data.frame(final_importance)

# Create comparison plots
p1 <- ggplot(head(importance_df, 20), aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Initial Top 20 Features",
       x = "Feature",
       y = "Gain")

p2 <- ggplot(head(final_importance_df, 20), aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Final Top 20 Features (After Selection)",
       x = "Feature",
       y = "Gain")

grid.arrange(p1, p2, ncol = 2)

# Print feature selection summary
cat("\nFeature Selection Summary:\n")
cat("Original number of features:", ncol(initial_train_matrix), "\n")
cat("Selected number of features:", length(selected_features), "\n")
cat("Percentage of features kept:", round(length(selected_features)/ncol(initial_train_matrix)*100, 2), "%\n")
```
:::

## b: Hard Classification (10 pts)

Suppose you are asked to make a hard classification using the
probabilities from part a. Making a false negative is 4 times worse that
making a false positive (i.e., $C_{FN} = 4*C_{FP}$).

-   What threshold should be used with your predictions? How did you
    choose?

::: {.callout-note title="Solution"}
> 0.2 is the threashold should be used with for prediction

We use the formula: threshold = 1/(1 + CFN/CFP) When CFN = 4\*CFP, the
optimal threshold is: P(Y=1\|X) \> 1/(1 + CFN/CFP) = 1/(1 + 4) = 0.2

```{r}
optimal_threshold <- 0.2
```
:::

-   How many of the offers in the test set are classified as *Accept*
    using this threshold?

::: {.callout-note title="Solution"}
> 216 offers are classified as accept using this threashold

```{r}
predictions <- submission

# Make classifications using the optimal threshold
classifications <- ifelse(predictions$prob_accept >= optimal_threshold, "Accept", "Reject")

# Count number of predicted accepts
num_accepts <- sum(classifications == "Accept")

# Create summary statistics
cat("Optimal threshold:", optimal_threshold, "\n")
cat("Number of predicted accepts:", num_accepts, "\n")
cat("Percentage of predicted accepts:", round(num_accepts/length(classifications)*100, 2), "%\n")

# Visualize prediction distribution with threshold
ggplot(predictions, aes(x = prob_accept)) +
  geom_histogram(binwidth = 0.05, fill = "lightblue", color = "black") +
  geom_vline(xintercept = optimal_threshold, color = "red", linetype = "dashed") +
  annotate("text", x = optimal_threshold + 0.1, y = max(hist(predictions$prob_accept, plot = FALSE)$counts),
           label = paste("Threshold =", optimal_threshold)) +
  labs(title = "Distribution of Predicted Probabilities",
       x = "Predicted Probability of Accept",
       y = "Count")
```
:::

## c. Feature Importance (10 pts)

What features are most important? Describe your results and approach in
a language that a clinician would want to listen to and can understand.
Be clear about the type of feature importance you used, the data
(training, testing) that was used to calculate the feature importance
scores, and the limitations inherent in your approach to feature
importance.

Notes:

-   Your audience is a non-data scientist, so be sure to give a brief
    high level description of any terms they may not be familiar with.
-   You wouldn't want to show the clinician the feature importance of
    all 100+ features. Indicate how to selected the *most* important
    features to report.
-   You are not expected to know the clinical meaning of the features.

::: {.callout-note title="Solution"}
> Based on our analysis of donor acceptance patterns from the data
> provided. We are ablet to identify a few key factors that strongly
> impact the surgery outcome. We used a specific technique called
> "Gain-based Feature Importance". Our model went over one hundred plus
> features but we have identified the top ten most influential factors
> in donor acceptance which could be found below. However, these factors
> shuold be considered correlation instead of direct causes for donor
> heart transpalnt acceptance as more medical research shuold be
> invested into find the causal relationships.

```{r}
# Get final feature importance from the model
final_importance <- xgb.importance(model = xgb_model)
final_importance_df <- as.data.frame(final_importance)

# Create a cleaner visualization for clinical audience
ggplot(head(final_importance_df, 10), aes(x = reorder(Feature, Gain), y = Gain * 100)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  theme(
    text = element_text(size = 12),
    axis.title = element_text(face = "bold"),
    plot.title = element_text(size = 14, face = "bold")
  ) +
  labs(
    title = "Top 10 Most Influential Factors in Donor Acceptance",
    x = "Factor",
    y = "Relative Importance (%)",
    caption = "Based on model training data"
  )
```
:::

## d. Calibration (10 pts)

Assess the calibration of your predictions. There are no points off for
having a poorly calibrated model; the purpose of this problem is to
demonstrate your knowledge of how to evaluate calibration.

::: {.callout-note title="Solution"}
```{r}
# Get predictions from training data
train_predictions <- predict(xgb_model, dtrain)

# Create data frame with predictions and actual values
calibration_df <- data.frame(
  pred_prob = train_predictions,
  actual = train_labels
)

# Create bins of predicted probabilities (10 bins)
calibration_df$bin <- cut(calibration_df$pred_prob, 
                         breaks = seq(0, 1, length.out = 11))

# Calculate calibration metrics for each bin
calibration_summary <- calibration_df %>%
  group_by(bin) %>%
  summarise(
    n = n(),
    mean_pred = mean(pred_prob),
    mean_actual = mean(actual),
    se = sqrt((mean_actual * (1 - mean_actual)) / n)
  )

# Create calibration plot
cal_plot <- ggplot(calibration_summary, aes(x = mean_pred, y = mean_actual)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  geom_errorbar(aes(ymin = mean_actual - 2*se, ymax = mean_actual + 2*se), width = 0.02) +
  geom_point(aes(size = n), color = "blue") +
  scale_x_continuous(limits = c(0, 1)) +
  scale_y_continuous(limits = c(0, 1)) +
  theme_minimal() +
  labs(
    title = "Calibration Plot",
    x = "Predicted Probability",
    y = "Observed Proportion",
    size = "Sample Size"
  ) +
  theme(legend.position = "bottom")

# Create reliability diagram
rel_plot <- ggplot(calibration_summary, aes(x = mean_pred)) +
  geom_bar(aes(y = n/sum(n)), stat = "identity", alpha = 0.3) +
  labs(
    title = "Reliability Diagram",
    x = "Predicted Probability",
    y = "Fraction of Samples"
  ) +
  theme_minimal()

# Display plots side by side
grid.arrange(cal_plot, rel_plot, ncol = 2)

# Calculate Brier score
brier_score <- mean((train_predictions - train_labels)^2)

# Calculate mean calibration error
mean_cal_error <- mean(abs(calibration_summary$mean_pred - calibration_summary$mean_actual))

# Print calibration metrics
cat("\nCalibration Metrics:\n")
cat("Brier Score:", round(brier_score, 4), "\n")
cat("Mean Calibration Error:", round(mean_cal_error, 4), "\n")
```
:::

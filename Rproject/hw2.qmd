---
title: "Homework #2: Resampling" 
author: "**Your Name Here**"
format: ds6030hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```


# Required R packages and Directories {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/teaching/data/' # data directory
library(tidymodels)# for optional tidymodels solutions
library(tidyverse) # functions for data manipulation  
```


# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Data Generating Process

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform between $0$ and $2$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.callout-note title="Solution"}
```{r}
# simulate x
sim_x <- function(n) runif(n, min = 0, max = 2)

# define true function
f <- function(x) 1+2*x+5*sin(5*x)

# simulate y 
sim_y <- function(x, sd) {
  n <- length(x)
  f(x) + rnorm(n, sd = sd)
}
```

:::

## b. Simulate data

Simulate $n=100$ realizations from these distributions. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. Use `set.seed(211)` prior to generating the data.

::: {.callout-note title="Solution"}

```{r}
# set number of simulations and sd
n = 100
sd = 2.5

# set seed
set.seed(211)

# simulate
x = sim_x(n)
y = sim_y(x, sd = sd)

# use tibble to form data
data <- tibble(x, y)

# plotting 
ggplot(data = data, aes(x,y)) +
  geom_point()+
  geom_function(fun = f, color = "blue")
```

:::


## c. 5th degree polynomial fit

Fit a 5th degree polynomial. Produce a scatterplot and draw the *estimated* regression curve.

::: {.callout-note title="Solution"}

```{r}
# fit the 5th poly model
fit5 <- lm(y~poly(x, 5))

# make estimates
xseq <- seq(0, 2, length = 200) 
yhat5 <- predict(fit5, tibble(x=xseq))

# make data for plotting and convert to long
pred.data <- tibble(
  x = xseq,
  y = yhat5
) %>%
  pivot_longer(
    cols = -x,
    names_to = "model",
    values_to = "y"
  )

# produce graphs
ggplot(data = tibble(x,y), aes(x=x, y=y)) + 
  geom_point() +
  geom_line(data = pred.data, color = "blue")
```


:::


## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.callout-note title="Solution"}

```{r}
# setting up 
M = 200 # number of bootstrap samples
set.seed(212) # setting seeds
n = 100
eval_pts = seq(0, 2, length = 100) 

# creating a list to store predictions from bootstrap
bootstrap_predictions <- matrix(NA, nrow = length(eval_pts), ncol = M)

# bootstrapping
for (m in 1:M){
  ind <- sample(n, replace = T) 
  data_boot <- data[ind, ]
  m_boot <- lm(y~poly(x, degree = 5), data = data_boot)
  bootstrap_predictions[, m] <- predict(m_boot, newdata = tibble(x = eval_pts))
}

# Convert bootstrap predictions into a data frame
bootstrap_df <- as_tibble(bootstrap_predictions) %>%
  mutate(eval_pts = eval_pts) %>%
  pivot_longer(cols = -eval_pts, names_to = "bootstrap", values_to = "y")

# Plot original data and bootstrap curves
ggplot(data = tibble(x, y), aes(x = x, y = y)) +
  geom_point() +
  geom_line(data = bootstrap_df, aes(x = eval_pts, y = y, group = bootstrap), color = "red", alpha = 0.3) +
  labs(title = "Bootstrap 5th Degree Polynomial Fits", x = "X", y = "Y")
```


:::
    
## e. Confidence Intervals

Calculate the pointwise 95% confidence intervals from the bootstrap samples. That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.callout-note title="Solution"}

```{r}
ci <- bootstrap_df |>
  group_by(eval_pts)|>
  summarise(
    lower = quantile(y, probs = 0.025),
    upper = quantile(y, probs = 0.975)
  )
  
ggplot(data = tibble(x, y), aes(x = x, y = y)) +
  geom_point() +
  geom_line(data = bootstrap_df, aes(x = eval_pts, y = y, group = bootstrap), color = "red", alpha = 0.3) +
  labs(title = "Bootstrap 5th Degree Polynomial Fits", x = "X", y = "Y") +
  geom_ribbon(data = ci, aes(x = eval_pts, ymin = lower, ymax = upper), alpha = 0.4, color = "blue", fill = "blue")
```

:::

# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Implement 10-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.callout-note title="Solution"}

```{r}
# generate data 
n = 100
sd = 2.5
set.seed(221)
x = sim_x(n)
y = sim_y(x, sd = sd)
data <- tibble(x, y)

############ 

library(tidymodels)

# create the 10 folds 
cv_folds <- vfold_cv(data, v = 10)

# make the model
knn_model <- nearest_neighbor(weight_func = "rectangular", neighbors = tune()) |>
  set_engine("kknn") |>
  set_mode("regression")

# make the recipe
knn_recipe <- recipe(y~x, data = data)

# create workflow
knn_workflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(knn_recipe)

############ 

# define grid to test
k_grid <- tibble(neighbors = 3:40)

# Tune the model using cross-validation and the defined grid
knn_tune_results <- tune_grid(
  knn_workflow,
  resamples = cv_folds,
  grid = k_grid,
  metrics = metric_set(rmse) 
)

############ 

# Collect the tuning results
results <- knn_tune_results |>
  collect_metrics(summarize = F) |>
  transmute(
    fold = id,
    k_neighbor = neighbors,
    mse = .estimate^2,
  ) |>
  arrange(fold, k_neighbor) |>
  group_by(k_neighbor) |>
  summarize(
    fold = n(),
    se = sd(mse)/sqrt(n),
    mse = mean(mse)
  ) |>
  arrange(mse)

head(results)

############ 

ggplot(data = results, aes(x = k_neighbor, y = mse)) + 
  geom_line() +
  geom_point()+
  geom_errorbar(aes(ymin = mse-se, ymax = mse+se), width = 0.2) 
```

:::


## b. Find the optimal *edf*

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. What is the optimal *edf*? Be sure to use the correct sample size when making this calculation. Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.callout-note title="Solution"}

```{r}
results = results %>% mutate(edf = (nrow(data)*9/10)/k_neighbor)
slice_min(results, mse)

#plotting 
ggplot(data = results, aes(x = edf, y = mse)) +
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin = mse-se, ymax = mse+se))
```

The optimal k is 9 with an estimated edf = 10
:::

## c. Choose $k$

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 

::: {.callout-note title="Solution"}

I would choose a k value of 9 because it has the most amount of edf while retaining the least amonut of mse as per part a and part b stated in the graphs

:::

## d. Evaluate actual performance

Now we will see how well cross-validation performed. Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the test set.

::: {.callout-note title="Solution"}

```{r}
#### I can't figure out how to use tidymodels for this so I wrote a function 
n = 50000
sd = 2.5
set.seed(223)
x = sim_x(n)
y = sim_y(x, sd = sd)
test_data <- tibble(x, y)

evaluate_knn <- function(train_data, test_data, k_values) {
  # Initialize a vector to store 
  mse_values <- numeric(length(k_values))
  
  # Loop 
  for (i in seq_along(k_values)) {
    neighbors <- k_values[i]
    
    # KNN regression
    knn_model <- FNN::knn.reg(
      train = select(train_data, x),
      y = train_data$y,
      test = select(test_data, x),
      k = neighbors
    )
    
    # residuals and mse
    residuals <- test_data$y - knn_model$pred
    mse_values[i] <- mean(residuals^2)
  }
  
  # to return
  tibble(k = k_values, mse = mse_values, n_eval = nrow(test_data))
}

test_result = evaluate_knn(data, test_data, k = 3:40) %>% 
  mutate(edf = nrow(data) / k)

test_result |> arrange(mse) |> head()
```
The optimal k is 11 and the edf is 9.09

:::

## e. Performance plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 
    
::: {.callout-note title="Solution"}

```{r}
bind_rows(
  cv = results %>% rename(k = k_neighbor), 
  test = test_result, 
  .id="error"
  ) %>% 
  ggplot(aes(edf, mse, color=error)) + 
  geom_point() + geom_line() +
  labs(title="EDF on x-axis")

bind_rows(
  cv = results %>% rename(k = k_neighbor), 
  test = test_result, 
  .id="error"
  ) %>% 
  ggplot(aes(k, mse, color=error)) + 
  geom_point() + geom_line() + 
  labs(title="k on x-axis")
```


:::
    
## f. Did cross-validation work as intended?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.callout-note title="Solution"}

Based on the plots from e it does appear that cv is working as intended. I'm not sure about the second part because I'm having a hard time reading and understanding the graphs. I will refer to the answer key later to review.
:::





---
title: "Homework #1: Supervised Learning"
author: "**Alex Yang**"
format: ds6030hw-html
---

```{r config}
#| include: false
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages and Directories {.unnumbered .unlisted}

```{r packages}
#| message: false
#| warning: false
library(tidyverse) # functions for data manipulation
```

# Problem 1: Evaluating a Regression Model

## a. Data generating functions

Create a set of functions to generate data from the following distributions:

```{=tex}
\begin{align*}
X &\sim \mathcal{N}(0, 1) \\
Y &= -1 + .5X + .2X^2 + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma)
\end{align*}
```
::: {.callout-note title="Solution"}
```{r}
sim_func <- function(n, sigma) {
  # generate X
  X <- rnorm(n, mean = 0, sd = 1)
  # generate epsilon
  epsilon <- rnorm(n, mean = 0, sd = sigma)
  # generate Y
  Y <- -1+0.5*X+0.2*X^2+epsilon
  # return elements
  tibble(X = X, Y = Y)
}
```
:::

## b. Generate training data

Simulate $n=100$ realizations from these distributions using $\sigma=3$. Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$.

-   Use `set.seed(611)` prior to generating the data.

::: {.callout-note title="Solution"}
```{r}
set.seed(611)
train_data <- sim_func(100, 3)
true_function <- function(X){
  -1+ 0.5*X + 0.2*X^2
}

ggplot(train_data, aes(x=X, y=Y)) + 
  geom_point() +
  geom_function(fun = true_function)
```
:::

## c. Fit three models

Fit three polynomial regression models using least squares: linear, quadratic, and cubic. Produce another scatterplot, add the fitted lines and true population line $f(x)$ using different colors, and add a legend that maps the line color to a model.

-   Note: The true model is quadratic, but we are also fitting linear (less complex) and cubic (more complex) models.

::: {.callout-note title="Solution"}
```{r}
# we have already generated training data train_data
# creating three models 
linear_model <- lm(Y ~X, data = train_data)
quad_model  <- lm(Y~poly(X, degree = 2, raw = T), data = train_data)
cube_model <- lm(Y~poly(X, degree = 3, raw = T), data = train_data)

# creating new data points to predict 
new_data <- tibble(X = seq(min(train_data$X), max(train_data$X), length=100))

# appending new results into new_data
new_data <- new_data %>%
  mutate(
    linear_pred = predict(linear_model, newdata = new_data),
    quad_pred = predict(quad_model, newdata = new_data),
    cube_pred = predict(cube_model, newdata = new_data)
  )

# generate ggplot
ggplot(train_data, aes(x=X, y=Y)) + 
  geom_point(color = 'black', alpha = 0.5, size = 0.3) +
  geom_function(fun = true_function, aes(color = "True Population Line")) +
  geom_line(data=new_data, aes(x=X, y=linear_pred, color = 'Linear'))+
  geom_line(data=new_data, aes(x=X, y=quad_pred, color = 'Quadratic'))+
  geom_line(data=new_data, aes(x=X, y=cube_pred, color = 'Cubic'))+
  scale_color_manual(name = "Model",
                     values = c("True Population Line" = "black", 
                                "Linear" = "red", 
                                "Quadratic" = "green", 
                                "Cubic" = "purple")) 
```
:::

## d. Predictive performance

Generate a *test data* set of 10,000 observations from the same distributions. Use `set.seed(612)` prior to generating the test data.

-   Calculate the estimated mean squared error (MSE) for each model.
-   Are the results as expected?

::: {.callout-note title="Solution"}
```{r}
set.seed(612)
train_data <- sim_func(10000, 3)

train_data <- train_data %>%
  mutate(
    linear_pred = predict(newdata = train_data, linear_model),
    quad_pred = predict(newdata = train_data, quad_model),
    cube_pred = predict(newdata = train_data, cube_model)
  )

mse_linear <- mean((train_data$Y - train_data$linear_pred)^2)
mse_quadratic <- mean((train_data$Y - train_data$quad_pred)^2)
mse_cubic <- mean((train_data$Y - train_data$cube_pred)^2)

cat("MSE for Linear Model: ", mse_linear, "\n")
cat("MSE for Quadratic Model: ", mse_quadratic, "\n")
cat("MSE for Cubic Model: ", mse_cubic, "\n")
```

The results are not to be expected because when the degrees increase the MSE increased as well. They should be less as the degree increases and the model becomes more flexible
:::

## e. Optimal performance

What is the best achievable MSE? That is, what is the MSE if the true $f(x)$ was used to evaluate the test set? How close does the best method come to achieving the optimum?

::: {.callout-note title="Solution"}
```{r}
# adding bestmse from the true function
train_data <- train_data |>
  mutate(
    bestmse = -1+0.5*X+0.2*X^2
  )

# calculate best mse 
best_mse = mean((train_data$Y-train_data$bestmse)^2)
cat("the best mse is: ", best_mse)
```
:::

## f. Replication

The MSE scores obtained in part *d* came from one realization of training data. Here will we explore how much variation there is in the MSE scores by replicating the simulation many times.

-   Re-run parts b. and c. (i.e., generate training data and fit models) 100 times.
    -   Do not generate new testing data
    -   Use `set.seed(613)` prior to running the simulation and do not set the seed in any other places.
-   Calculate the test MSE for all simulations.
    -   Use the same test data from part d. (This question is only about the variability that comes from the *training data*).
-   Create kernel density or histogram plots of the resulting MSE values for each model.

::: {.callout-note title="Solution"}
```{r}
# define simulation number 
sim_num = 100

# make a tibble to store results
library(tidyverse)
mse_f <- tibble(
  mse_linear = numeric(sim_num),
  mse_quad = numeric(sim_num),
  mse_cube = numeric(sim_num)
)

set.seed(613)

# repeating parts
for (x in 1:sim_num){
  # generate new data
  new_data <- sim_func(100, 3)
  
  # fit new models
  linear_model <- lm(Y ~X, data = new_data)
  quad_model  <- lm(Y~poly(X, degree = 2, raw = T), data = new_data)
  cube_model <- lm(Y~poly(X, degree = 3, raw = T), data = new_data)
  
  # predict using train_data
  train_data <- train_data %>%
    mutate(
      linear_pred = predict(linear_model, newdata = train_data),
      quad_pred = predict(quad_model, newdata = train_data),
      cube_pred = predict(cube_model, newdata = train_data)
    )
  
  # calculating MSE
  mse_f$mse_linear[x] <- mean((train_data$Y - train_data$linear_pred)^2)
  mse_f$mse_quad[x] <- mean((train_data$Y - train_data$quad_pred)^2)
  mse_f$mse_cube[x] <- mean((train_data$Y - train_data$cube_pred)^2)
}

# transform the long table to short
mse_f_long <- mse_f %>%
  pivot_longer(cols=everything(), names_to = "model", values_to = "mse")

# plotting
ggplot(data = mse_f_long, aes(x = mse, fill=model)) +
  geom_density(alpha = 0.4)
```
:::



## g. Best model

Show a count of how many times each model was the best. That is, out of the 100 simulations, count how many times each model had the lowest MSE.

::: {.callout-note title="Solution"}
```{r}
mse_f <- mse_f %>%
  rowwise() %>%
  mutate(best_model = case_when(
    mse_linear == min(c(mse_linear, mse_quad, mse_cube)) ~ "Linear",
    mse_quad == min(c(mse_linear, mse_quad, mse_cube)) ~ "Quadratic",
    mse_cube == min(c(mse_linear, mse_quad, mse_cube)) ~ "Cubic"
  ))

# conuting
best_model_count <- mse_f %>%
  group_by(best_model) %>%
  summarise(count = n())

best_model_count
```
:::

## h. Function to implement simulation

Write a function that implements the simulation in part *f*. The function should have arguments for i) the size of the training data $n$, ii) the standard deviation of the random error $\sigma$, and iii) the test data. Use the same `set.seed(613)`.

::: {.callout-note title="Solution"}
```{r}
simulate_mse <- function(n, sigma, test_data, sim_num = 100){
  # create tibble
  mse_f <- tibble(
    mse_linear = numeric(sim_num),
    mse_quad = numeric(sim_num),
    mse_cube = numeric(sim_num)
  )
  
  # set seed
  set.seed(613)
  
  # Repeating the simulation
  for (x in 1:sim_num) {
    # Generate new training data of size n with the specified sigma
    new_data <- sim_func(n, sigma)
    
    # Fit models to the new training data
    linear_model <- lm(Y ~ X, data = new_data)
    quad_model  <- lm(Y ~ poly(X, degree = 2, raw = TRUE), data = new_data)
    cube_model  <- lm(Y ~ poly(X, degree = 3, raw = TRUE), data = new_data)
    
    # Predict on the fixed test data (test_data)
    test_data <- test_data %>%
      mutate(
        linear_pred = predict(linear_model, newdata = test_data),
        quad_pred = predict(quad_model, newdata = test_data),
        cube_pred = predict(cube_model, newdata = test_data)
      )
    
    # Calculate MSE for each model on the test data
    mse_f$mse_linear[x] <- mean((test_data$Y - test_data$linear_pred)^2)
    mse_f$mse_quad[x] <- mean((test_data$Y - test_data$quad_pred)^2)
    mse_f$mse_cube[x] <- mean((test_data$Y - test_data$cube_pred)^2)
  }
  
  return(mse_f)
}
```
:::

## i. Performance when $\sigma=2$s

Use your function to repeat the simulation in part *f*, but use $\sigma=2$. Report the number of times each model was best (you do not need to produce any plots).

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 2$, using `seed = 612`).

::: {.callout-note title="Solution"}
```{r}
# generate new test_data
set.seed(612)
test_data_sigma2 <- sim_func(1000, 2)

# run the simulation 
mse_results_sigma2 <- simulate_mse(n=100, sigma = 2, test_data = test_data_sigma2)

# copying parts from g
mse_results_sigma2 <- mse_results_sigma2 %>%
  rowwise() %>%
  mutate(best_model = case_when(
    mse_linear == min(c(mse_linear, mse_quad, mse_cube)) ~ "Linear",
    mse_quad == min(c(mse_linear, mse_quad, mse_cube)) ~ "Quadratic",
    mse_cube == min(c(mse_linear, mse_quad, mse_cube)) ~ "Cubic"
  ))

# count
best_model_count_sigma2 <- mse_results_sigma2 %>%
  group_by(best_model) %>%
  summarise(count = n())

best_model_count_sigma2

```
:::

## j. Performance when $\sigma=4$ and $n=300$

Repeat *i*, but now use $\sigma=4$ and $n=300$.

-   Be sure to generate new test data with ($n = 10000$, $\sigma = 4$, using `seed = 612`).

::: {.callout-note title="Solution"}
```{r}
set.seed(612)
test_data_sigma4 <- sim_func(10000, 4)

# run sim
mse_results_sigma4 <- simulate_mse(n = 300, sigma = 4, test_data = test_data_sigma4)

mse_results_sigma4 <- mse_results_sigma4 %>%
  rowwise() %>%
  mutate(best_model = case_when(
    mse_linear == min(c(mse_linear, mse_quad, mse_cube)) ~ "Linear",
    mse_quad == min(c(mse_linear, mse_quad, mse_cube)) ~ "Quadratic",
    mse_cube == min(c(mse_linear, mse_quad, mse_cube)) ~ "Cubic"
  ))

best_model_count_sigma4 <- mse_results_sigma4 %>%
  group_by(best_model) %>%
  summarise(count = n())

best_model_count_sigma4
```
:::

## k. Understanding

Describe the effects $\sigma$ and $n$ has on selection of the best model? Why is the *true* model form (i.e., quadratic) not always the *best* model to use when prediction is the goal?

::: {.callout-note title="Solution"}
When $\sigma$ becomes larger the data becomes more random and this allows simpler models like linear models perform better because they simply don't overfit as much as the complicated models. 

When $n$ or sample size is small like we have seen in part c, more complicated models might overfit models and produce a less accurate model while simple models like the linear model performs better. However when the sample size increase largely (for example from 100 to 10000), the more complicated models will start performing better bceause overfitting is reduced. 

The reason that the true model form is not always the best model is because depending on the data sample size and randomness (or noise and data limitation) a simpler model might reduce prediction error by minimizing the overfitting issue. 
:::
